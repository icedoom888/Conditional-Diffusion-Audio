# V2 and after has the CLAP embeddings fixed
training:
  output_dir: V2_init_image
  data_root: ../vits/LJSProcessedFull
  vits_root: ../vits
  wandb_project: natural_voices
  wandb_log_every: 50
  train_batch_size: 8
  eval_batch_size: 4
  num_epochs: 20
  save_images_epochs: 1
  save_model_epochs: 1
  gradient_accumulation_steps: 4
  loss_fn_diffusion: l2
  learning_rate: 1.e-4
  lr_scheduler: constant
  lr_warmup_steps: 500
  adam_beta1: 0.95
  adam_beta2: 0.999
  adam_epsilon: 1.e-8
  CFG_mask_proba: 0.1
  adam_weight_decay: 1.e-6
  use_ema: true
  ema_inv_gamma: 1
  ema_power: 0.75
  ema_max_decay: 0.999
  mixed_precision: "fp16"
  start_epoch: 0
  num_train_steps: 0.5e+4
model:
  channels: [128, 256, 512, 512, 1024]
  factors: [1, 2, 2, 2, 2]
  layers: [1, 2, 2, 2, 2]
  attentions: [0, 0, 0, 1, 1]
  cross_attentions: [0, 0, 0, 1, 1]
  attention_heads: 4
  attention_features: 64
  use_additional_time_conditioning: false
  use_initial_image: true
  use_data_mask: true
  use_embedding_cfg: true
  embedding_max_length: 64
  embedding_features: 512


